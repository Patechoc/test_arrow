{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing the Apache Parquet Format\n",
    "\n",
    "## Existing open standards\n",
    "- XML, json\n",
    "- SQL\n",
    "- binary storage format with metadata (NetCDF, HDF5, Apache Parquet)\n",
    "- serialization/ RPC protocols (Apache AVRO, protocol buffers)\n",
    "\n",
    "\n",
    "## Why we need open standards?\n",
    "Performance, no overhead, valid accross programming language\n",
    "\n",
    "## Which benefits for Pandas?\n",
    "Not based originally on open standards, that's it!\n",
    "\n",
    "## Why columnar tables?\n",
    "- SQL is row oriented format (ex. Apache Impala, PostgreSQL)\n",
    "- but queries are often made on columns of a table, or on a subset of the columns.\n",
    "\n",
    "## The Apache Arrow project\n",
    "- **Goal**: Define an open standard for column-oriented tables (data frames) that is language-independant (Java, Python, R, Javascript, ...), so **portable accross languages**\n",
    "\n",
    "- need specifications, libraies, tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Apache Parquet format: https://github.com/apache/parquet-format\n",
    "* videos: \n",
    "  * 2018-07 by Wes McKinney: https://www.youtube.com/watch?v=y7zGnKzaKIw (existing standards, challenges)\n",
    "  * 2019-06 by Wes McKinney: https://www.youtube.com/watch?v=uZA55cGDaBQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Single Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({'one': [-1, np.nan, 2.5],\n",
    "                   'two': ['foo', 'bar', 'baz'],\n",
    "                   'three': [True, False, True]},\n",
    "                  index=list('abc'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write this to Parquet format with write_table:\n",
    "import pyarrow.parquet as pq\n",
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a single Parquet file.\n",
    "# In practice, a Parquet dataset may consist of many files\n",
    "# in many directories.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read a single file back with read_table:\n",
    "table2 = pq.read_table('example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a subset of columns to read,\n",
    "# which can be much faster than reading the whole file\n",
    "# (due to the columnar layout):\n",
    "pq.read_table('example.parquet', columns=['one', 'three']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reading a subset of columns from a file that used\n",
    "# a Pandas dataframe as the source,\n",
    "# we use read_pandas to maintain any additional index column data:\n",
    "pq.read_pandas('example.parquet', columns=['two']).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitting the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'one': [-1, np.nan, 2.5],\n",
    "                   'two': ['foo', 'bar', 'baz'],\n",
    "                   'three': [True, False, True]},\n",
    "                  index=list('abc'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, 'example_noindex.parquet')\n",
    "t = pq.read_table('example_noindex.parquet')\n",
    "t.to_pandas()\n",
    "# Here you see the index did not survive the round trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
